---
title: "CASE Workshop 2025: Shortcourse"
output: html_document
date: "2025-06-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
We will be using Google Colab to implement this deep extreme quantile regression example. First, go to [https://colab.research.google.com/](https://colab.research.google.com/). Click File -> New notebook in Drive, and then change the runtime to R (Runtime -> Change runtime type, then pick R in the dropdown). We will not be using GPUs, so keep the CPU box checked.



# Installation

To install Keras, run the following code. Colab already has Python and Tensorflow modules installed, so we do not need to do anything particularly complicated here. If installing Keras for R on your own machine, see the tutorial here [https://tensorflow.rstudio.com/install/](https://tensorflow.rstudio.com/install/). 


```{r eval = F}
install.packages(c("keras3","lime","evd"))
```

```{r include = F}
reticulate::use_virtualenv("r-keras")
```

Now, check to see if Keras has installed correctly.
```{r }
library("keras3")
```

Set seeds for reproducibility.
```{r}
set.seed(1)
tensorflow::set_random_seed(1)
```

## Extracting a dataset

We will be downloading a dataset of US wildfires from my R package, which is hosted on github. The data are described in [Richards and Huser (2022)](https://arxiv.org/abs/2208.07581) and also [here](https://github.com/Jbrich95/pinnEV/blob/main/R/USWild.R).

We take only the last 5 years of the dataset (there's seven months per year, so 7*5 = 35 observations).

```{r}
#file_url <- "https://github.com/Jbrich95/pinnEV/blob/main/data/USWild.rda?raw=true"
#load(url(file_url))
load("USWild.rda")
Y = USWild$BA[127:161, , ]
X = array(dim = c(35, 129, 61, 22))
X[, , , 1] = USWild$X$X.t2m[127:161, , , ]
X[, , , 2] = USWild$X$X.SPI[127:161, , , ]
X[, , , -c(1, 2)] = USWild$X$X.N[127:161, , , 1:20]
```

The data are arranged on a regular spatial grid, and we have 22 covariates. The first two covariates are monthly air temperature (t2m) are standardised precipitation index (SPI); the remaining covariates are a mixture of meteorological and land cover descriptors.
```{r}
dim(X)
cov.names <- c("t2m", "SPI", USWild$X$cov.names[1:20])
print(cov.names) #Prints names of covariates
```

The response is monthly square-root burnt area due to wildfires in the US. Burnt area data are *very* heavy-tailed, so taking the square root is going to make modelling a bit more numerically stable.
```{r}
image(log(1 + Y[1, , ]), col = heat.colors(20, rev = T), asp = 0.625)
```

We can also look at some of the covariates. Note that these maps are for the first month in the dataset.

```{r fig.height =3, fig.width = 6}
par(mfrow = c(1, 2))
image(X[1, , , 1],
      col = heat.colors(20, rev = T),
      main = "temperature",
      asp = 0.625)
image(X[1, , , 2],
      col = terrain.colors(20, rev = F),
      main = "SPI",
      asp = 0.625)
```

We will model extremes of positive $Y$, i.e., square-root burnt area. Note that I'm completely removing all zero values - we won't be needing these here - and now our data are stored vectors, rather than arrays.
```{r}
X.positive <- apply(X, 4, function(x) x[Y > 0])
Y.positive <- sqrt(Y[Y > 0])
```

When it comes to ML modelling, training is more numerically stable if your data are standardised/normalised. Here, we scale the covariates to have zero mean, unit variance.

```{r}
means <- apply(X.positive, 2, mean)
sds <- apply(X.positive, 2, sd)
X.scaled <- apply(as.matrix(1:ncol(X.positive)), 1, function(ind)
  (X.positive[, ind] - means[ind]) / sds[ind])
```

# Standard prediction

We now build and train a standard prediction model with Keras. To account for the heavy-tailedness of the response, we will target the expected log of $Y$ using the mean-squared error loss function.

For simplicity, we will sample 45000 observations for training and 5000 for testing.

```{r}
n = length(Y.positive)
train_sample_idx <- sample(1:n, 45000)
test_sample_idx <- sample((1:n)[-train_sample_idx], 5000)
Y_train <- Y.positive[train_sample_idx]
X_train <- X.scaled[train_sample_idx, ]
Y_test <- Y.positive[test_sample_idx]
X_test <- X.scaled[test_sample_idx, ]
```

We first show how to build Keras models sequentially. Here we have two hidden layers, each of width 64, with ReLU activation functions. The final layer has a linear activation function:

```{r}
model <- keras_model_sequential()

model %>%
  
  # Adds a densely-connected layer with 64 units to the model:
  layer_dense(units = 64, activation = 'relu') %>%
  
  # Add another:
  layer_dense(units = 64, activation = 'relu') %>%
  
  
  # Add a final layer with 1 output
  layer_dense(units = 1, activation = 'linear')
```

Now, compile the model with a loss function and an optimizer. Here we use Adam with standard hyper-parameters, and the MSE loss function to do regular prediction. We also evaluated the mean absolute error, and track its value during training.

```{r}
model %>% compile(
  optimizer = "adam",
  loss = "mean_squared_error",
  metrics = list("mean_absolute_error")
)
```


Now fit the model. We train the model for 100 epochs, with an 80/20 validation data split. The default minibatch size is 16. Note that the model gradients are not evaluated on the validation data, and instead we can use the validation loss (i.e., the loss evaluated on the validation data) to motivate hyperparameters (e.g., neural net architecture) choices.

If you choose to cancel training (ctrl + C, or the big red stop button), then the current model state will be saved and accessible. Feel free to set `verbose = 1` to track the training.

```{r}
history <- model %>% fit(
  x = as.matrix(X_train),
  y = as.matrix(log(Y_train)),
  epochs = 100,
  validation_split = 0.2,
  verbose = 0,
  shuffle = T
)
```
Plot the training history, and print the summary of the architecture.
```{r}
plot(history)
summary(model)
```

We can see the the model begins to overfit quickly. Let's re-run the model with a checkpointed training scheme; the model loss will be evaluated at each epoch, and will be saved if it provides the best fit on the validation data. In this way, we can re-load the model state/checkpoint which best generalised to unseen data (in this case, the validation data).

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'linear') %>% 
  compile( optimizer = "adam", loss = "mean_squared_error", 
           metrics = list("mean_absolute_error"))

checkpoint <- callback_model_checkpoint(
  paste0("checkpoints/LSE.weights.h5"),
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  save_weights_only = TRUE,
  mode = "min",
  save_freq = "epoch"
)

history <- model %>% fit( x = X_train,
                          y = as.matrix(log(Y_train)),
                          epochs=50,
                          batch_size=16,
                          verbose = 0,
                          callbacks = list(checkpoint),
                          validation_split=0.2)
```

```{r}
plot(history)
```

Now, load that best saved checkpoint:
```{r}
model <- load_model_weights(model,
                                 filepath = paste0("checkpoints/LSE.weights.h5"))
```

We now get the predictions from the fitted model, using the standard `predict` function.

```{r}
predictions <- model %>% predict(X_test)
```

```{r}
plot(log(Y_test), predictions)
abline(a = 0, b = 1)
```

For comparsion, we can also fit a linear model. This can be built and trained in Keras as well; recall that a linear model is a special case of a one-layered MLP with one neuron and with a linear final activation layer.
```{r}
linear.model <- keras_model_sequential() %>%
  layer_dense(units = 1, activation = 'linear') %>%
  compile(
    optimizer = "adam",
    loss = "mean_squared_error",
    metrics = list("mean_absolute_error")
  )


history <- linear.model %>% fit(
  x = X_train,
  y = as.matrix(log(Y_train)),
  epochs = 15,
  verbose = 0,
  validation_split = 0.2
)
```

This model trains much faster than the MLP...
```{r}
plot(history)
```

Here we plot the out-of-sample predictions. As you would expect, the model performs much more poorly than the deep neural network model.

```{r}
linear_predictions <- linear.model %>% predict(X_test)
plot(log(Y_test), linear_predictions, asp = 1)
abline(a = 0, b = 1)
```

We may also want to see how the predictions change with the covariates. Below, we plot maps of the predicted $\log(Y)$ for a specific month $t$. Note that we have to use the original covariates here, rather than just those conditional on $Y>0$; this means we also need to remember to scale our original covariate vector, in the same was as we did above for the subset of covariates used in the model.

```{r fig.height =3, fig.width = 9}
par(mfrow = c(1, 3))

t <- 1
X.plot <- X[t, , , ]
dim(X.plot) <- c(prod(dim(X.plot)[1:2]), dim(X.plot)[3])
# Apply scaling
X.plot.scaled <- apply(as.matrix(1:ncol(X.plot)), 1, function(ind)
  (X.plot[, ind] - means[ind]) / sds[ind])

linear.predictions <- linear.model %>% predict(X.plot.scaled)
dim(linear.predictions) <- dim(Y)[2:3]
linear.predictions[Y[t, , ] < 0] = NA 
#linear.predictions[Y[t, , ] <= 0] = NA #Comment for US-wide prediction
image(log(Y[t, , ]), main = "Observation",asp=0.625)
image(linear.predictions, main = "Linear model predictions",asp=0.625)

NN.predictions <- model %>% predict(X.plot.scaled)
dim(NN.predictions) <- dim(Y)[2:3]
NN.predictions[Y[t, , ] < 0] = NA 
#NN.predictions[Y[t, , ] <= 0] = NA #Comment for US-wide prediction
image(NN.predictions, main = "MLP model predictions",asp=0.625)

```



# Quantile regression

Now, we will perform quantile regression using deep neural networks. We will target the $\tau = 0.8$ quantile of $Y| \mathbf{X}$ by using the pinball loss function. We will supply this as a custom loss function to Keras.

All loss functions in Keras take two in puts: `y_true`, the true values of your response data, and `y_pred`, the predicted values that are outputted from your neural network model. Note that `y_pred` does not need to be directly connected to $Y$, e.g., the expectation or a quantile; below, `y_pred` will correspond to the GPD parameters.

Loss functions must be differentiable. To ensure this is the case, we build them using backend Keras functions. This can sometimes make it quite difficult to write custom loss functions in Keras; for these examples, the loss functions are quite simple, but you may need to get a bit creative if your loss function uses non-standard operations!
```{r}
tau <- 0.85 # Set quantile level
tilted_loss <- function(y_true, y_pred) {

  error = y_true - y_pred
  return(op_mean(op_maximum(tau * error, (tau - 1) * error)))
}
```

Now, we use the same hidden layer model, but target the 80% quantile of Y. 

```{r}
u.model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu')  
```

This time, I specify an exponential activation in the final layer, to ensure that the quantile is strictly positive; recall that we removed all non-positive values of the response.  
```{r}
u.model <- u.model %>% layer_dense(
  units = 1,
  activation = "exponential")

```

We compile and fit the model. This time, I supply a larger minibatch size, in order to speed-up training slightly.
```{r}
u.model %>%
  compile(optimizer = "adam", loss = tilted_loss)

checkpoint <- callback_model_checkpoint(
  paste0("checkpoints/u.weights.h5"),
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  save_weights_only = TRUE,
  mode = "min",
  save_freq = "epoch"
)

history <- u.model %>% fit(
  x = X_train,
  y = as.matrix(Y_train),
  batch_size = 64,
  epochs = 60,
    verbose = 0,
  callbacks = list(checkpoint),
  validation_split = 0.2
)
```

```{r}
plot(history)
```

Now load that best checkpoint:
```{r}
u.model <- load_model_weights(u.model,
                                 filepath = paste0("checkpoints/u.weights.h5"))
```

Evaluate the test predictions of the $\tau$-quantile. As a sanity check, we will evalute the proportion of test data that exceed the predicted quantile; this should be close to $1-\tau$.

```{r}
test.pred.theshold <- u.model %>% predict(X_test)
mean(Y_test < test.pred.theshold) # Should be close to tau
```

We now plot maps of the estimated $\tau$-quantile for a specified month. Below, we take the third month, but feel free to change $t$ in the block of code.

```{r fig.height =3, fig.width = 6}
par(mfrow = c(1, 2))

t <- 3
X.plot <- X[t, , , ]
dim(X.plot) <- c(prod(dim(X.plot)[1:2]), dim(X.plot)[3])
# Apply scaling
X.plot.scaled <- apply(as.matrix(1:ncol(X.plot)), 1, function(ind)
  (X.plot[, ind] - means[ind]) / sds[ind])

image(log(Y[t, , ]), main = "Observation", asp = 0.625)

NN.predictions <- u.model %>% predict(X.plot.scaled)
dim(NN.predictions) <- dim(Y)[2:3]
NN.predictions[Y[t, , ] < 0] = NA
#NN.predictions[Y[t, , ] <= 0] = NA #Comment to provide US-wide predictions.
image(log(NN.predictions), main = "Threshold prediction", asp = 0.625)

```

# Non-stationary POT modelling

We now fit a deep GPD model to excesses above our estimated quantile.

First, get your excesses and create a test dataset. We will use the same train/test split as before.
```{r}
train.pred.threshold <- u.model %>% predict(X_train)

train.exceed_idx = which(Y_train > train.pred.threshold)

Y_train.excess = (Y_train - train.pred.threshold)[train.exceed_idx]
X_train.excess = X_train[train.exceed_idx, ]

test.exceed_idx = which(Y_test > test.pred.theshold)

Y_test.excess = (Y_test - test.pred.theshold)[test.exceed_idx]
X_test.excess = X_test[test.exceed_idx, ]
length(Y_train.excess)
length(Y_test.excess)
```

As we have much less data, we will build a slightly simpler model. This one uses sigmoid activation functions, rather than ReLU. The activation functions are a hyperparameter of your model that will need to be optimised.

```{r}
GPD.model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'sigmoid') %>%
  layer_dense(units = 32, activation = 'sigmoid') 

```

This time, the final activation layer has two hidden units, not one: one each for sigma and xi. We use an exponential activation function to ensure that they are both strictly positive.
```{r}
GPD.model <- GPD.model %>% layer_dense(units = 2, activation = "exponential")
```

Now, let's write the custom loss function. This will correspond to the negative log-likelhood associated with the GPD threshold excesses.

```{r}

# Note that we are treating the first dimension of the output (y_pred) as sigma, and the second as xi
GPD_nll <- function(y_true, y_pred) {

  sigma = y_pred[all_dims(), 1] #I apply the all_dims() function here to ensure that everything is a vector
  xi = y_pred[all_dims(), 2]
  y = y_true[all_dims(), 1]
  #Evaluate log-likelihood
  ll1 = -(1 / xi + 1) * op_log(1 + xi * y / sigma)
  
  ll2 = -op_log(sigma)
  
  return(-(op_sum(ll1 + ll2)))
}

```

We now compile ad train our model. This time, I'm also going to perform early-stopping of the training. We will end the training early if the validation loss has not decreased in 10 epochs; this saves computational expense that may otherwise be wasted.
```{r}
GPD.model %>%
  compile(optimizer = "adam", loss = GPD_nll)

checkpoint <- callback_model_checkpoint(
  paste0("checkpoints/GPD.weights.h5"),
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  save_weights_only = TRUE,
  mode = "min",
  save_freq = "epoch"
)

early.stopping <-   callback_early_stopping(monitor = "val_loss", patience = 10)

history <- GPD.model %>% fit(
  x = X_train.excess,
  y = as.matrix(Y_train.excess),
  batch_size = 16,
  epochs = 100,
  verbose = 0,
  callbacks = list(checkpoint, early.stopping),
  validation_split = 0.2
)

plot(history)
```


Now, load that best checkpoint:
```{r}
GPD.model <- GPD.model %>% load_model_weights(
                                 filepath = paste0("checkpoints/GPD.weights.h5"))
```

We will save the test negative log-likelihood and use this for model comparison.
```{r}
test_loss_GPD1 <- GPD.model %>% evaluate(X_test.excess,
                                         as.matrix(Y_test.excess),
                                         batch_size = length(Y_test.excess))
print(paste0("test loss for GPD NN: ", test_loss_GPD1))
```

Get your test predictions:

```{r}
test.GPD.estimates <- GPD.model %>% predict(X_test.excess)
test.sigma = test.GPD.estimates[, 1]
test.xi = test.GPD.estimates[, 2]
par(mfrow = c(1, 2))
hist(test.sigma, main = "sigma estimates")
hist(test.xi, main = "xi estimates")
```

To assess model fit, we will look at a pooled QQ plot. First, transform all excesses to unit exponential margins. Then, create a QQ plot for data on unit exponential margins.
```{r}
exp.data = apply(cbind(Y_test.excess, test.sigma, test.xi), 1, function(x) {
  qexp(evd::pgpd(
    x[1],
    loc = 0,
    scale = x[2],
    shape = x[3]
  ))
})

n_p = length(exp.data)
ps = (1:n_p) / (n_p + 1)
qs = quantile(exp.data, ps, na.rm = T)

plot(
  qexp(ps),
  qs,
  ylab = "Fitted",
  xlab = "Theoretical",
  main = "MLP model 1",
  pch = 20,
  asp = 1
)
abline(a = 0, b = 1, col = "red")

```



Now, let's look at maps of the estimated scale and shape parameters.
```{r fig.height =3, fig.width = 9}
par(mfrow = c(1, 3))

t <- 3
X.plot <- X[t, , , ]
dim(X.plot) <- c(prod(dim(X.plot)[1:2]), dim(X.plot)[3])
# Apply scaling
X.plot.scaled <- apply(as.matrix(1:ncol(X.plot)), 1, function(ind)
  (X.plot[, ind] - means[ind]) / sds[ind])

image(log(Y[t, , ]), main = "Observation", asp = 0.625)

NN.predictions <- GPD.model %>% predict(X.plot.scaled)
sigma.predictions <- NN.predictions[, 1]
dim(sigma.predictions) <- dim(Y)[2:3]
sigma.predictions[Y[t, , ] < 0] = NA
#sigma.predictions[Y[t, , ] <= 0] = NA
image(sigma.predictions, main = "sigma prediction", asp = 0.625)

xi.predictions <- NN.predictions[, 2]
dim(xi.predictions) <- dim(Y)[2:3]
xi.predictions[Y[t, , ] < 0] = NA
#xi.predictions[Y[t, , ] <= 0] = NA
image(xi.predictions, main = "xi prediction", asp = 0.625)
```

The MLP model fits the data well, but can we improve on things?

## More parsimonious modelling

Three things in life are certain: death, taxes, and one of your reviewers questioning your modelling choices for $\xi$. Here we consider two simpler models for $\xi(\mathbf{x})$; one with much less covariate effect and one with $\xi(\mathbf{x})=\xi$ constant.

In the first case, we constrain $\xi(\mathbf{x})$ to be less than one and greater than zero, and let it depend on only the first covariate in `\mathbf{x}`; this is the air temperature. To allow this, we need to build the Keras model in a non-sequential manner, and specfically define its input layers.

Note that, when not building the Keras model sequentially, you need to specify the input dimension!
```{r}
input_sigma <- layer_input(shape = ncol(X_train.excess), name = 'input_sigma')
input_xi <- layer_input(shape = 1, name = 'input_xi')
```

We can now pipe the input layers into separate models for the two parameters. The models here do not share parameters nor architecture; in fact, even the modality of the data can differ between the two models!
```{r}
sigma_model <- input_sigma %>%
  layer_dense(units = 32, activation = 'sigmoid') %>%
  layer_dense(units = 32, activation = 'sigmoid') %>%
  layer_dense(units = 1, activation = 'exponential')
xi_model <- input_xi %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid') 
```

Now, concatenate (join) the two models together. This will be our output later, i.e., `y_pred`. We also need to tell Keras what are the inputs and outputs:
```{r}
output <- layer_concatenate(c(sigma_model, xi_model))
GPD.model2 <- keras_model(inputs = c(input_sigma, input_xi), outputs = output)

GPD.model2 %>%
  compile(optimizer = "adam", loss = GPD_nll)

checkpoint <- callback_model_checkpoint(
  paste0("checkpoints/GPD2.weights.h5"),
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  save_weights_only = TRUE,
  mode = "min",
  save_freq = "epoch"
)
```
Let's train this model. Here, we supply the covariates `x` as a named list; we're using the same names for the list entries as we have specified in `output`.
```{r}
history <- GPD.model2 %>% fit(
  x = list(
    "input_sigma" = X_train.excess,
    "input_xi" = as.matrix(X_train.excess[, 1])
  ),
  y = as.matrix(Y_train.excess),
  batch_size = 16,
  epochs = 100,
  verbose = 0,
  callbacks = list(checkpoint, early.stopping),
  validation_split = 0.2
)
plot(history)

GPD.model2 <- GPD.model2 %>% load_model_weights(filepath = paste0("checkpoints/GPD2.weights.h5"))

#Get predictions 
test.GPD.estimates <- GPD.model2 %>% predict(list(X_test.excess, as.matrix(X_test.excess[, 1])))
test.sigma = test.GPD.estimates[, 1]
test.xi = test.GPD.estimates[, 2]
par(mfrow = c(1, 2))
hist(test.sigma, main = "sigma estimates")
hist(test.xi, main = "xi estimates")
```

How well does this model fit?
```{r}
exp.data = apply(cbind(Y_test.excess, test.sigma, test.xi), 1, function(x) {
  qexp(evd::pgpd(
    x[1],
    loc = 0,
    scale = x[2],
    shape = x[3]
  ))
})

n_p = length(exp.data)
ps = (1:n_p) / (n_p + 1)
qs = quantile(exp.data, ps, na.rm = T)

plot(
  qexp(ps),
  qs,
  ylab = "Fitted",
  xlab = "Theoretical",
  main = "MLP model 2",
  pch = 20,
  asp = 1
)
abline(a = 0, b = 1, col = "red")

```


```{r}
test_loss_GPD2 <-  GPD.model2 %>% evaluate(
  list(X_test.excess, as.matrix(X_test.excess[, 1])),
  as.matrix(Y_test.excess),
  batch_size = length(Y_test.excess)
)
print(paste0("test loss for GPD NN2: ", test_loss_GPD2))
```

Map your predictions

```{r fig.height =3, fig.width = 9}
par(mfrow = c(1, 3))

t <- 3
X.plot <- X[t, , , ]
dim(X.plot) <- c(prod(dim(X.plot)[1:2]), dim(X.plot)[3])
# Apply scaling
X.plot.scaled <- apply(as.matrix(1:ncol(X.plot)), 1, function(ind)
  (X.plot[, ind] - means[ind]) / sds[ind])

image(log(Y[t, , ]), main = "Observation", asp = 0.625)

NN.predictions <- GPD.model2 %>% predict(list(X.plot.scaled, as.matrix(X.plot.scaled[, 1])))
sigma.predictions <- NN.predictions[, 1]
dim(sigma.predictions) <- dim(Y)[2:3]
sigma.predictions[Y[t, , ] < 0] = NA
#sigma.predictions[Y[t, , ] <= 0] = NA
image(sigma.predictions, main = "sigma prediction", asp = 0.625)

xi.predictions <- NN.predictions[, 2]
dim(xi.predictions) <- dim(Y)[2:3]
xi.predictions[Y[t, , ] < 0] = NA
#xi.predictions[Y[t, , ] <= 0] = NA
image(xi.predictions, main = "xi prediction", asp = 0.625)
```

For the last model, we will make $\xi(\mathbf{x})$ constant. We achieve this by making `input_xi` a vector of ones.
```{r}
xi_model_fixed <- input_xi %>%
  layer_dense(
    units = 1, use_bias = F,
 activation = 'exponential') 

```
Now, join the two models together and define the inputs and outputs:
```{r}
GPD.model3 <- keras_model(inputs = c(input_sigma, input_xi),
                          outputs = c(layer_concatenate(c(
                            sigma_model, xi_model_fixed
                          ))))
```
Note that, in the above, we are reusing `sigma_model`. We have not redefined `sigma_model`, so its weights/biases have already been ``pre-trained''.


Now, compile and train the final GPD model.
```{r}
GPD.model3 %>%
  compile(optimizer = "adam", loss = GPD_nll)

checkpoint <- callback_model_checkpoint(
  paste0("checkpoints/GPD3.weights.h5"),
  monitor = "val_loss",
  verbose = 0,
  save_best_only = TRUE,
  save_weights_only = TRUE,
  mode = "min",
  save_freq = "epoch"
)
history <- GPD.model3 %>% fit(
  x = list(
    "input_sigma" = X_train.excess,
    "input_xi" = as.matrix(rep(1,nrow(X_train.excess)))
  ),
  y = as.matrix(Y_train.excess),
  batch_size = 16,
  epochs = 100,
  verbose=0,
  callbacks = list(checkpoint,early.stopping),
  validation_split = 0.2
)
plot(history)
GPD.model3 <- GPD.model3 %>% load_model_weights(
                                 filepath = paste0("checkpoints/GPD3.weights.h5"))
```

Get your predictions:

```{r}
test.GPD.estimates <- GPD.model3 %>% predict(list(X_test.excess, as.matrix(rep(1, nrow(X_test.excess)))))
test.sigma = test.GPD.estimates[, 1]
test.xi = test.GPD.estimates[1, 2]
hist(test.sigma, main = "sigma estimates")
print(paste0("xi estimate: ", test.xi))
```

Assess your model fit:

```{r}
exp.data = apply(cbind(Y_test.excess, test.sigma, test.xi), 1, function(x) {
  qexp(evd::pgpd(
    x[1],
    loc = 0,
    scale = x[2],
    shape = x[3]
  ))
})

n_p = length(exp.data)
ps = (1:n_p) / (n_p + 1)
qs = quantile(exp.data, ps, na.rm = T)

plot(
  qexp(ps),
  qs,
  ylab = "Fitted",
  xlab = "Theoretical",
  main = "MLP model 3",
  pch = 20,
  asp = 1
)
abline(a = 0, b = 1, col = "red")

```

Now, let's compare all four models. We should find that model 3 is the preferred, and provides the best test loss.
```{r}
test_loss_GPD3 <-  GPD.model3 %>% evaluate(
  list(X_test.excess, as.matrix(rep(1,nrow(X_test.excess)))),
  as.matrix(Y_test.excess),
  batch_size = length(Y_test.excess)
)

print(paste0("test loss for GPD NN1: ", test_loss_GPD1))
print(paste0("test loss for GPD NN2: ", test_loss_GPD2))
print(paste0("test loss for GPD NN3 (homogeneous xi): ", test_loss_GPD3))
```
How does a linear model compare?
```{r}
GPD.linear.model <- keras_model_sequential() %>%
  layer_dense(units = 2, activation = 'exponential')
GPD.linear.model %>%
  compile(optimizer = "adam", loss = GPD_nll)

history <- GPD.linear.model %>% fit(
  x = X_train.excess,
  y = as.matrix(Y_train.excess),
  batch_size = 16,
  epochs = 40,
  verbose = 0,
  validation_split = 0.2
)
plot(history)
test_loss_linear_GPD <-  GPD.linear.model %>% evaluate(X_test.excess,
                                                 as.matrix(Y_test.excess),
                                                 batch_size = length(Y_test.excess))
print(paste0("test loss for linear GPD: ", test_loss_linear_GPD))


```


Here are our final mapped estimates for the last MLP model. 


```{r fig.height =3, fig.width = 6}
par(mfrow = c(1, 2))

t <- 3
X.plot <- X[t, , , ]
dim(X.plot) <- c(prod(dim(X.plot)[1:2]), dim(X.plot)[3])
# Apply scaling
X.plot.scaled <- apply(as.matrix(1:ncol(X.plot)), 1, function(ind)
  (X.plot[, ind] - means[ind]) / sds[ind])

image(log(Y[t, , ]), main = "Observation", asp = 0.625)

NN.predictions <- GPD.model3 %>% predict(list(X.plot.scaled, as.matrix(rep(1, nrow(X.plot.scaled)))))
sigma.predictions <- NN.predictions[, 1]
dim(sigma.predictions) <- dim(Y)[2:3]
sigma.predictions[Y[t, , ] < 0] = NA
#sigma.predictions[Y[t, , ] <= 0] = NA
image(log(sigma.predictions), main = "sigma prediction", asp = 0.625)

xi.prediction <- NN.predictions[1, 2]
print(paste("GPD model 3, estimated xi: ", xi.prediction))
```

# Interpretability via LIME

We now perform a post-hoc interpretability assessment of the final GPD model using Local Interpretable Model-Agnostic Explanations (LIME). 

LIME works by finding a liner model that maps the covariates to the predictions from the MLP. In this case, the prediction will correspond to the GPD log-scale parameter. For one covariate sample of interest, say $\mathbf{x}^*$, we can create synthetic data by perturbing the values (by, e.g., adding noise to $\mathbf{x}^*$). For each synthetic data point, we find the predicted GPD log-scale parameter. Then, we perform weighted OLS to find a linear model that maps from the synthetic data to the predictions; synthetic values close to $\mathbf{x}^*$ are given more weight.

We will use the `lime` R package. Below, I define a model class and prediction function to allow `lime` to interact with our Keras model. The predict_ function will output a data frame with the log of the scale parameter that is outputted from the Keras predict function.

```{r}
library(lime)

class (GPD.model3)
model_type.keras.src.models.functional.Functional <- function(x, ...) {
  "regression"
}

predict_model.keras.src.models.functional.Functional <- function (x, newdata, type, ...) {
  pred <- x %>% predict(list(as.matrix(newdata), as.matrix(rep(0, nrow(newdata)))))
  
  data.frame(log(pred[, 1]))
}

```

Now, create an explainer object with our training data.
```{r}
df.train = as.data.frame(X_train.excess)
names(df.train) <- cov.names
explainer <- lime::lime (x              = df.train,
                         model          = GPD.model3,
                         bin_continuous = F)

df.test = as.data.frame(X_test.excess)
names(df.test) <- cov.names
```


We can then get our explanations. Here I'm getting an explanation for the first 20 test covariate vectors. I will only focus on the top 10 most important features.
```{r}
explanation <- lime::explain (df.test[1:20, ], n_features = 10, explainer    = explainer)

plot_explanations (explanation)
```

We can also focus in on specific covariate vectors. Here, I look at the explanations for the four largest burnt areas in the test dataset.  
```{r}
inds = order(Y_test.excess, decreasing = T)
explanation <- lime::explain (df.test[inds[1:4], ], n_features = 6, explainer    = explainer)
plot_features (explanation)
```