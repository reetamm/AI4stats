{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSTYCrZ8LYiq26Ys+9kdk1"},"kernelspec":{"name":"ir","display_name":"R"},"language_info":{"name":"R"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","We will be using Google Colab for this example on density regression using semi-parametric quantile regression (SPQR). First, go to https://colab.research.google.com/. Click File -> New notebook in Drive, and then change the runtime to R (Runtime -> Change runtime type, then pick R in the dropdown). We will not be using GPUs, so keep the CPU box checked.\n","\n","# Installation\n","To install Keras3, run the following code. Colab already has Python and Tensorflow modules installed, so we do not need to do anything particularly complicated here."],"metadata":{"id":"3U_VMApTEWc_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fCMpUosEPdG"},"outputs":[],"source":["remotes::install_github(\"rstudio/tensorflow\")\n","install.packages(c(\"keras3\",\"splines2\"))\n","library(keras3)"]},{"cell_type":"markdown","source":["Set seed for reproducibility."],"metadata":{"id":"9L_EwS76Ek6a"}},{"cell_type":"code","source":["set_random_seed(1)"],"metadata":{"id":"BJ5QrHEJEmeA","executionInfo":{"status":"ok","timestamp":1761758975404,"user_tz":300,"elapsed":35316,"user":{"displayName":"Reetam Majumder","userId":"01351925884765990765"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## The model\n","\n","We will assume, that for a covariate vector $X$ (which could be multivariate) and a univariate response $Y$, $$Y_i\\vert X_i \\sim Normal(\\mu(X_i),\\sigma(X_i))$$\n","\n","and estimate the conditional density of $Y\\vert X$. We first demonstrate this using some simulated data, and then use the weather data use used for SPQR.\n","\n","## Generating data\n","\n","We generate 10000 data points from the following data generating process:\n","$$X_1\\sim Beta(3,2), X_2\\sim Beta(2,5)$$\n","$$Y_i\\vert X_{1i},X_{2i} \\sim Normal(X_{1i}^2 - 3X_{2i},2X_{1i})$$\n","\n","We also generate 1000 test points.\n"],"metadata":{"id":"JD9_-Z6uAWDx"}},{"cell_type":"code","source":["x_train <- cbind(rbeta(10000,3,2), rbeta(10000,2,5))\n","y_train <- rnorm(10000,mean = x_train[,1]^2-3*x_train[,2]+5,sd = 2*x_train[,1])\n","x_test <- cbind(rbeta(1000,3,2), rbeta(1000,2,5))\n","y_test <- rnorm(1000,mean = x_test[,1]^2-3*x_test[,2]+5,sd = 2*x_test[,1])"],"metadata":{"id":"OOm5JaiJAioM","executionInfo":{"status":"ok","timestamp":1761758975458,"user_tz":300,"elapsed":31,"user":{"displayName":"Reetam Majumder","userId":"01351925884765990765"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize this real quick."],"metadata":{"id":"iUxhnaHrAlxT"}},{"cell_type":"code","source":["par(mfrow=c(1,3))\n","plot(x_train[,1],y_train)\n","plot(x_train[,2],y_train)\n","plot(density(y_train))\n","par(mfrow=c(1,1))"],"metadata":{"id":"9LJUKn4nAnWM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All of this is really non-linear and I don't think linear regression would be a good idea here. Quantile regression may(?) be better? But we will estimate the conditional density instead."],"metadata":{"id":"pQx5MGRhAqEy"}},{"cell_type":"markdown","source":["## Define the keras model\n","\n","This is no longer a sequential model, because we need to run our 2 outputs (the conditional mean and SD) through different activation functions. We employ what is called the functional API for `keras3`. A detailed guide on how it differs from the sequential models is found in the official docs at <https://keras3.posit.co/articles/functional_api.html>.\n","\n","In short, things don't need to be in sequence and can have different branches, where we can do different things to different branches. Complex architectures will often be built using the functional API."],"metadata":{"id":"U1TMEbCnAsJz"}},{"cell_type":"code","source":["input1 <- keras_input(shape=dim(x_train)[2], name = 'covariates')\n","x_1 <- layer_dense(input1, units = 12, activation = 'relu')\n","\n","x_2 <- layer_dense(x_1, 12, activation = 'relu')\n","\n","mu <- layer_dense(x_2, 1, activation = 'linear', name = \"mean\")\n","sig <- layer_dense(x_2, 1, activation = 'exponential', name = \"sd\")\n","\n","out_concat <- layer_concatenate(mu,sig)\n","out_concat <- layer_identity(out_concat, name='params')\n","model <- keras_model(inputs = list(input1),\n","                     outputs = out_concat, name = \"norm_dist\")"],"metadata":{"id":"Sgig54atAtcq","executionInfo":{"status":"ok","timestamp":1761758976202,"user_tz":300,"elapsed":67,"user":{"displayName":"Reetam Majumder","userId":"01351925884765990765"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### The loss function\n","\n","The output layer of the MLP above is not a point prediction of $Y$, but instead the conditional mean and SD of a Normal distribution fitted to $Y\\vert X$. We will now do MLE. For this we need the log-likelihood of the Normal distribution. In particular, since this needs to be a loss function that is minimized, we have the negative log-likelihood."],"metadata":{"id":"D_K7kbQMAwzm"}},{"cell_type":"code","source":["nloglik_loss_normal  = function (y_true, y_pred){\n","    # print(numbasis)\n","    mu <- y_pred[,1]\n","    sig <- y_pred[,2]\n","    isthisloss <- op_sum(op_log(sig) + 0.5*((y_true-mu)/sig)**2)\n","    return(isthisloss)\n","}"],"metadata":{"id":"gk6yJ8ZoAypw","executionInfo":{"status":"ok","timestamp":1761758998817,"user_tz":300,"elapsed":20,"user":{"displayName":"Reetam Majumder","userId":"01351925884765990765"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["I now compile and run the model. I have very few bells and whistles in my model specification; I am sure this can be improved with some effort."],"metadata":{"id":"edQbc7ewA0OT"}},{"cell_type":"code","source":["model |> compile(\n","    loss = nloglik_loss_normal,\n","    optimizer = optimizer_adam(learning_rate=0.01)\n",")\n","\n","history <- model |> fit(\n","    x= x_train,\n","    y=y_train,\n","    epochs = 50,\n","    batch_size = 128,\n","    verbose=0,\n","    callbacks=list(callback_early_stopping(monitor = \"val_loss\",\n","                                                     min_delta = 0, patience = 10)),\n","    validation_split = 0.2\n",")"],"metadata":{"id":"y_ViCAbCA3kK","executionInfo":{"status":"ok","timestamp":1761759019377,"user_tz":300,"elapsed":9685,"user":{"displayName":"Reetam Majumder","userId":"01351925884765990765"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Plot the history and the model:"],"metadata":{"id":"AaISy3FVA6Fa"}},{"cell_type":"code","source":["model\n","plot(history)"],"metadata":{"id":"nXIY7Jh9A7mF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It seems to have converged pretty quickly - even though this is non-linear, by deep learning standards, it is fairly straightforward..\n","\n","## Predictions\n","\n","Since I know what the acutal means and SDs were for this, we can directly compare."],"metadata":{"id":"mspI259LA_Qu"}},{"cell_type":"code","source":["preds <- as.matrix(model(x_test))\n","true_mean <- x_test[,1]^2-3*x_test[,2]+5\n","true_sd <- 2*x_test[,1]\n","\n","par(mfrow=c(1,2))\n","plot(preds[,1],true_mean)\n","abline(0,1)\n","\n","plot(preds[,2],true_sd)\n","abline(0,1)\n","par(mfrow=c(1,1))"],"metadata":{"id":"Wnvcm5RQBBBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fitting a distribution for `tmax|pr`\n","\n","I'm going to get the same data I used for the SPQR example. I'll even keep the same model I used with the simulated data. The only change I'll make is I will add an intercept term to the precipitation data so that there are 2 covariates which is what the model is expecting. It also let to a better fitted and stable model when I checked."],"metadata":{"id":"ZWEBVZZaBE_y"}},{"cell_type":"code","source":["file_url <- \"https://github.com/reetamm/AI4stats/blob/main/weather.RDS?raw=true\"\n","weather <- readRDS(url(file_url))\n","weather <- weather[weather$month==7 & weather$loc==1,]\n","head(weather)\n","\n","mnth <- weather$month\n","tmax <- weather$tmax - 273 #convert tmax to celsius\n","pr <- log(weather$pr + 0.0001) #convert pr to log-scale\n","plot(tmax,pr,pch=20)\n","n_total <- length(tmax)\n","train_ind <- sample(1:n_total,ceiling(0.8*n_total)) #80% training data\n","\n","tmax_range <- range(tmax)\n","y1 <- tmax\n","X <- cbind(1,pr)\n","\n","# train and validation data\n","y1_train <- y1[train_ind]\n","y1_test <- y1[-train_ind]\n","\n","# For conditional density of tmax with intercept and log-pr\n","X2_train <- X[train_ind,1:2]\n","X2_test <- X[-train_ind,1:2]"],"metadata":{"id":"xg_PW7kQBGIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now fit the model with this new dataset."],"metadata":{"id":"xKcZVCV-Baoy"}},{"cell_type":"code","source":["history <- model |> fit(\n","    x= X2_train,\n","    y=y1_train,\n","    epochs = 50,\n","    batch_size = 128,\n","    verbose = 0,\n","    callbacks=list(callback_early_stopping(monitor = \"val_loss\",\n","                                           min_delta = 0, patience = 10)),\n","    validation_split = 0.2\n",")\n","plot(history)"],"metadata":{"id":"sHfWhfQ-BbJz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predictions\n","For each data point, we will be predicting out a mean and SD. There aren't any `true` values to compare again, but we can nevertheless check goodness of fit. I'll do that for the first 6 observations."],"metadata":{"id":"p3ndoJjLBeBM"}},{"cell_type":"code","source":["preds <- as.matrix(model(X2_test))\n","head(y_test)\n","head(preds)\n","y_pred <- matrix(NA,1000,6)\n","for(i in 1:6)\n","    y_pred[,i] <- rnorm(1000,preds[i,1],preds[i,2])\n","par(mfrow=c(2,3))\n","for(i in 1:6){\n","    prcp <- round(exp(X2_test[i,2]) - 0.0001,2)\n","    plot(density(y_pred[,i]),main=paste0('prcp = ',prcp))\n","    abline(v=y1_test[i])\n","}\n","par(mfrow=c(1,1))"],"metadata":{"id":"J0t7LecRBhJu"},"execution_count":null,"outputs":[]}]}