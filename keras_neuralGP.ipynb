{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTtg3GljUYES+boraiNVvB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reetamm/AI4stats/blob/main/keras_neuralGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We will be using Google Colab for building a neural estimator for the range parameter of a Gaussian process with a squared exponential kernel.\n",
        "\n",
        "First, go to https://colab.research.google.com/. Click File -> New notebook in Drive, and then change the runtime to R (Runtime -> Change runtime type, then pick R in the dropdown). We will not be using GPUs, so keep the CPU box checked.\n",
        "\n",
        "# Installation\n",
        "To install Keras3, run the following code. Colab already has Python and Tensorflow modules installed, so we do not need to do anything particularly complicated here."
      ],
      "metadata": {
        "id": "3U_VMApTEWc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct_UokNcNoWL"
      },
      "outputs": [],
      "source": [
        "remotes::install_github(\"rstudio/tensorflow\")\n",
        "install.packages(c(\"keras3\",\"splines2\",\"mvtnorm\"))\n",
        "library(keras3)\n",
        "library(ggplot2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seed for reproducibility."
      ],
      "metadata": {
        "id": "BnrntxJ-N9o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pYG6DF55UZXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow::set_random_seed(1)"
      ],
      "metadata": {
        "id": "evTe_VMdN-Rp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading up the weather data we have been using"
      ],
      "metadata": {
        "id": "O-2IZl9TOpCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_url <- \"https://github.com/reetamm/AI4stats/blob/main/weather.RDS?raw=true\"\n",
        "weather <- readRDS(url(file_url))\n",
        "head(weather)"
      ],
      "metadata": {
        "id": "IvdaPuX5OFx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsetting the `tmax` data for January, and following up with a very crude plot."
      ],
      "metadata": {
        "id": "xIYokZSYOstc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weatherJan <- weather[weather$month==1 & weather$lat == weather$lat[1],]\n",
        "years <- lubridate::year(weatherJan$time)\n",
        "head(weatherJan)\n",
        "dim(weatherJan)\n",
        "ggplot(weatherJan[1:5,],aes(x=lon,y=lat,fill=tmax)) + geom_raster()"
      ],
      "metadata": {
        "id": "y1AkYxrMOKLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could stick with the original coordinates but I'll scale it because I have a better sense of what the range (lengthscale for the ML people) parameter should be if I have the distances on $(0,1)$. We first calculate the distance matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "7t_SKP3SPAyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coords <- weatherJan[1:5,3]\n",
        "coords_scaled <- (coords - min(coords))/diff(range(coords))\n",
        "d = abs(outer(coords_scaled,coords_scaled,\"-\")) # compute distance matrix, d_{ij} = |x_i - x_j|"
      ],
      "metadata": {
        "id": "tF9ZByWkPFBy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then set a prior on the dependence parameter, here denoted by $\\rho$. I'm just generating them from a $Unif(0,0.5)$"
      ],
      "metadata": {
        "id": "dzrXppxEPXaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K <- 50000\n",
        "n <- 64\n",
        "\n",
        "rho_train = runif(K,0,0.5) # length scale"
      ],
      "metadata": {
        "id": "LhAh3PzjOMgV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate some training data. We use $n=64$ replicates as this will coincide with the number of observations we have.\n",
        "\n",
        "We select the data to be on uniform margins using `pnorm`; but we could stick with any arbitrary margin."
      ],
      "metadata": {
        "id": "3bg5VYXpPqww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gp_val <- function(l,d,n){\n",
        "    y <- matrix(NA,length(l),n*5)\n",
        "    for(i in 1:length(l)){\n",
        "        Sigma_SE = exp(-d^2/(2*l[i]^2)) # squared exponential kernel\n",
        "        y[i,] = c(mvtnorm::rmvnorm(n,sigma=Sigma_SE))\n",
        "    }\n",
        "return(y)\n",
        "}\n",
        "x <- pnorm(gp_val(rho_train,d,n))\n",
        "\n",
        "dim(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Dr7Va3anON8y",
        "outputId": "a0522522-b595-473d-f8fd-45747a4eece9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>50000</li><li>320</li></ol>\n"
            ],
            "text/markdown": "1. 50000\n2. 320\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 50000\n\\item 320\n\\end{enumerate*}\n",
            "text/plain": [
              "[1] 50000   320"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the estimator"
      ],
      "metadata": {
        "id": "g3zH-h4fP7NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model <- keras_model_sequential()\n",
        "\n",
        "model %>%\n",
        "    # Adds a densely-connected layer with 64 units to the model:\n",
        "    layer_dense(units = 128, activation = 'relu') %>%\n",
        "\n",
        "    # Add another:\n",
        "    layer_dense(units = 64, activation = 'relu') %>%\n",
        "\n",
        "\n",
        "    # Add a final layer with 1 ouput\n",
        "    layer_dense(units = 1, activation = 'sigmoid')"
      ],
      "metadata": {
        "id": "4cfTpoAVQIVl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, compile the model with a loss function and an optimizer. Here we use Adam with standard hyper-parameters, and the MSE loss function to target the posterior mean."
      ],
      "metadata": {
        "id": "19mxtjrcQAYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model %>% compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = \"mean_squared_error\"\n",
        ")"
      ],
      "metadata": {
        "id": "xvL0zj9AQF_E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now fit the model. We train the model for 100 epochs, with an 80/20 validation data split. The default minibatch size is 16. Note that the model gradients are not evaluated on the validation data, and instead we can use the validation loss (i.e., the loss evaluated on the validation data) to motivate hyperparameters (e.g., neural net architecture) choices.\n",
        "\n",
        "If you choose to cancel training (ctrl + C, or the big red stop button), then the current model state will be saved and accessible."
      ],
      "metadata": {
        "id": "25jbd9NtQOz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early.stopping <-   callback_early_stopping(monitor = \"val_loss\", patience = 10)\n",
        "\n",
        "history <- model %>% fit(\n",
        "    x = x,\n",
        "    y = as.matrix(rho_train),\n",
        "    callbacks = list(early.stopping),\n",
        "    epochs = 100,\n",
        "    verbose = 0,\n",
        "    validation_split = 0.2,\n",
        "    shuffle = T\n",
        ")"
      ],
      "metadata": {
        "id": "6kN3J6RfORHa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training history, and print the summary of the architecture."
      ],
      "metadata": {
        "id": "gH-YaNlMQZDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(history)\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "qSJr-zobQaZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see how well the estimator performs. We generate 1000 test datasets and compare the true values of $\\rho$ with the predictions."
      ],
      "metadata": {
        "id": "BiFm5yX1QcGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K.test <- 1000\n",
        "rho_test <- runif(K.test,0,0.5)\n",
        "\n",
        "x.test <- pnorm(gp_val(rho_test,d,n))\n",
        "\n",
        "predictions <- model %>% predict(x.test)\n",
        "\n",
        "plot(rho_test, predictions)\n",
        "abline(a = 0, b = 1)"
      ],
      "metadata": {
        "id": "NRT0-c7lOXEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get annual maxima\n",
        "\n",
        "Now, we apply the estimator to real data. We will analyse annual temperature maxima.\n",
        "\n",
        "I calculate the monthly mean for each year. This would do better with a Gaussian process than just using the maxima."
      ],
      "metadata": {
        "id": "xwm3qJbGQqo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather3 <- aggregate(weatherJan$tmax,by=list(weatherJan$loc,years),FUN=mean)"
      ],
      "metadata": {
        "id": "3p-v8YvtOZUL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to put the data in the same form as the training data, and then scale it. We could fit a parametric/semiparametric model to transform the data to Uniform, but I'm doing the simplest thing in the form of a Z-transform and using `pnorm` on it.\n",
        "\n",
        "and then we can just estimate $\\rho$ for our data."
      ],
      "metadata": {
        "id": "ltEIB2eARfH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.test <- matrix(c(weather3[,3]),nrow = 1)\n",
        "x.test <- pnorm((x.test-mean(x.test))/sd(x.test))\n",
        "model %>% predict(x.test)"
      ],
      "metadata": {
        "id": "KvOL8DdTRUfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}